---
title: "üá©üá™ Verbesserung der deutschen Suche im Amazon OpenSearch Service" 
author: "Alexey Vidanov" 
date: 2023-12-08
toc: true 
draft: false 
image: "img/2023/12/opensearch-improved.png" 
thumbnail: "img/2023/12/opensearch-improved.png" 
categories: ["aws"] 
tags: [ "aws", "opensearch", "level-400", "deutsche suche", "unternehmenssuche", "deutsch" ]
---

Der Amazon OpenSearch Service, der auf dem robusten OpenSearch-Framework basiert, zeichnet sich durch seine bemerkenswerte Geschwindigkeit und Effizienz in Such- und Analysefunktionen aus. Trotz seiner St√§rken sind die Standardkonfigurationen des Dienstes m√∂glicherweise nicht vollst√§ndig darauf ausgelegt, die spezifischen sprachlichen Herausforderungen bestimmter Sprachen zu bew√§ltigen.

<!--more-->

{{% notice note %}}
For the English version of this article, please use [this link](https://www.tecracer.com/blog/2023/12/enhancing-german-search-in-amazon-opensearch-service.html).
{{% /notice %}}

Nehmen wir zum Beispiel das Deutsche, bekannt f√ºr seine zusammengesetzten W√∂rter wie ‚Äû**Lebensversicherungsgesellschaft**‚Äú. Standardm√§√üige Tokenisierung in Suchtechnologien behandelt diese Zusammensetzungen als einzelne Einheiten, was zu weniger optimalen Suchergebnissen f√ºhrt. F√ºr eine verbesserte Genauigkeit ist es wichtig, die Bestandteile dieser Zusammensetzungen separat zu indizieren ‚Äì ‚Äû**Leben**‚Äú, ‚Äû**Versicherung**‚Äú und ‚Äû**Gesellschaft**‚Äú. Dieser Ansatz stellt pr√§zisere und effektivere Suchergebnisse sicher, insbesondere in Sprachen wie Deutsch mit vielen zusammengesetzten W√∂rtern.

![Verbesserung der deutschen Suche im Amazon OpenSearch Service](/img/2023/12/better-search-german.png)

<!--more-->

## Kombination traditioneller Suche mit erweiterten Filtern

Stand Dezember 2023 unterst√ºtzt OpenSearch eine Reihe von Sprachoptionen f√ºr die `analyzer`-Funktion. Diese Sprachen umfassen: `arabisch`, `armenisch`, `baskisch`, `bengalisch`, `brasilianisch`, `bulgarisch`, `katalanisch`, `tschechisch`, `d√§nisch`, `niederl√§ndisch`, `englisch`, `estnisch`, `finnisch`, `franz√∂sisch`, `galicisch`, `deutsch`, `griechisch`, `hindi`, `ungarisch`, `indonesisch`, `irisch`, `italienisch`, `lettisch`, `litauisch`, `norwegisch`, `persisch`, `portugiesisch`, `rum√§nisch`, `russisch`, `sorani`, `spanisch`, `schwedisch`, `t√ºrkisch` und `thail√§ndisch`.

Wenn man jedoch den deutschen Analyzer auf unser obiges Beispiel anwendet, wird deutlich, dass er bei zusammengesetzten W√∂rtern Schwierigkeiten hat, diese effektiv in einfachere, suchbare Elemente zu zerlegen.

```json
GET _analyze
{
  "analyzer":"german",
  "text": ["Lebensversicherungsgesellschaft."]
}
```

Das Ergebnis ist jedoch nur ein Token: `lebensversicherungsgesellschaft`. Der eingebaute deutsche Analyzer wandelt die Eingabe in Kleinbuchstaben um und entfernt Stoppw√∂rter wie "und", "oder", "das", die f√ºr die Suche nicht wesentlich sind. Anschlie√üend wird eine Stammformreduktion durchgef√ºhrt, um W√∂rter suchbarer zu machen. Leider wird die Komplexit√§t der deutschen Sprache dabei nicht ausreichend ber√ºcksichtigt.

Um diese Herausforderung zu bew√§ltigen, wenden Entwickler oft `n-grams` an. Diese Methode zerlegt den Text in kleinere Teile einer bestimmten Gr√∂√üe. Zum Beispiel ergibt der Satz ‚Äû**Die Suche ist herausfordernd**‚Äú bei Anwendung von 3-5 Grammen Tokens:

*die, suc, such, suche, uch, uche, che, her, hera, herau, era, erau, eraus, rau, raus, rausf, aus, ausf, ausfo, usf, usfo, usfor, sfo, sford, sforder, for, ford, forde, order, ordern, der, dern, ern*

Obwohl dies helfen kann, f√ºhrt es oft zu vielen falschen Positiven. Es erzeugt zahlreiche bedeutungslose (z.B. her, ern, che) oder irref√ºhrende Tokens (z.B. ford, order, ordern), was zu aufgebl√§hten Indizes und erh√∂hter Clusterlast f√ºhrt. Dies wirkt sich auf die Suchpr√§zision und Betriebskosten aus.

Die Integration von `dictionary_decompounder` und `synonym`-Filtern bietet einen verfeinerten Ansatz. Diese Filter erh√∂hen die Pr√§zision und Effizienz bei der Verarbeitung deutscher Zusammensetzungen, indem sie diese in einfachere Tokens zerlegen. Zus√§tzlich erweitert die Synonymfunktionalit√§t die Reichweite der Suche, indem sie unterschiedliche Ausdrucksformen √§hnlicher Konzepte erkennt, was die Suchgenauigkeit und -umfassendheit weiter verbessert.

## Implementierung verbesserter Filter im Amazon OpenSearch

Der Prozess der Einrichtung dieser Filter ist unkompliziert und f√ºhrt zu einer deutlich verbesserten Sucherfahrung. Die Dekompoundierungsfilter sind hervorragend geeignet, um komplexe Zusammensetzungen zu zerlegen, w√§hrend die Synonymfilter die Suchf√§higkeiten erweitern, um verschiedene Ausdrucksformen √§hnlicher Konzepte einzubeziehen.

## Voraussetzungen

- **Amazon OpenSearch Service Cluster**: Sie sollten einen Amazon OpenSearch Service Cluster eingerichtet und betriebsbereit haben. Wenn Sie nicht sicher sind, wie Sie dies tun, bietet Amazon eine [umfassende Anleitung](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/create-cluster.html) daf√ºr.
- **Zugang zur AWS Management Console**: Sie ben√∂tigen Zugang zur AWS Management Console mit den erforderlichen Berechtigungen, um den Amazon OpenSearch Service zu verwalten.

Stellen Sie sicher, dass Sie diese Voraussetzungen erf√ºllt haben, bevor Sie mit den Schritten zur Implementierung von Dekompoundierungs- und Synonymfiltern f√ºr die deutsche Sprachsuche fortfahren.

### Schritt 1. Beschaffung der W√∂rterb√ºcher

Um Dekompoundierungsfilter effektiv zu implementieren, beginnen Sie mit der Beschaffung oder Erstellung einer Wortliste. F√ºr eine allgemeine Textdekompoundierung im Deutschen sollten Sie die L√∂sung von Uwe Schindler und Bj√∂rn Jacke in Betracht ziehen. Diese L√∂sung ist auf GitHub verf√ºgbar und bietet eine Wortliste [hier](https://raw.githubusercontent.com/uschindler/german-decompounder/master/dictionary-de.txt) und Trennungsregeln [hier](https://github.com/uschindler/german-decompounder/blob/master/de_DR.xml). Hinweis: Um sie mit Amazon OpenSearch Service Cluster zu verwenden, entfernen Sie die zweite Zeile in der Datei, die mit `<!DOCTYPE` beginnt.

F√ºr Synonyme nutzen Sie die Datei von Openthesaurus.de, verf√ºgbar [hier](https://github.com/PSeitz/germansynonyms/blob/master/german.syn). Um sie f√ºr OpenSearch anzupassen, ersetzen Sie Leerzeichen durch Kommas.

Um Ihnen die Verwendung dieser Dateien zu erleichtern, habe ich sie entsprechend angepasst und im Repository hochgeladen.

1. [de_DR.xml](https://github.com/vidanov/tecracer-blog-projects/blob/main/opensearch_german_search/de_DR.xml) ist eine Datei mit Trennungsregeln.
2. [german-decompound.txt](https://github.com/vidanov/tecracer-blog-projects/blob/main/opensearch_german_search/german-decompound.txt) ist das deutsche W√∂rterbuch f√ºr Dekompoundierung.
3. [german_synonym.txt](https://github.com/vidanov/tecracer-blog-projects/blob/main/opensearch_german_search/german_synonym.txt) ist ein Synonymw√∂rterbuch.

Beachten Sie Lizenzvereinbarungen, falls Sie diese Dateien verwenden m√∂chten.

### Schritt 2: Hinzuf√ºgen von W√∂rterb√ºchern zum Amazon OpenSearch Service

![Adding dictionaries to Amazon OpenSearch Service](/img/2023/12/image-20231125190208837.png)

1. Erstellen Sie einen S3-Bucket und laden Sie die Dateien hoch.
2. Greifen Sie auf die AWS-Konsole Ihres verwalteten OpenSearch-Clusters zu.
3. Registrieren Sie Ihre Pakete √ºber den Link "Pakete".
4. Verkn√ºpfen Sie die Pakete mit Ihrem OpenSearch-Cluster.

Hinweis: Sie k√∂nnen diesen Schritt mit IaC-Tools wie Terraform oder CDK automatisieren.

### Schritt 3: Erstellung eines Index mit einem benutzerdefinierten deutschen Analyzer in OpenSearch

Nachdem Sie die notwendigen Pakete erhalten haben, k√∂nnen Sie diese in Ihren OpenSearch-Index integrieren, indem Sie ihre jeweiligen Paket-IDs verwenden. Stellen Sie sicher, dass Sie die Platzhalter-IDs (`FYYYYYYY` f√ºr *german_synonym.txt* und `FXXXXXXX` f√ºr *german-decompound.txt* und `FZZZZZZZ` f√ºr *de_DR.xml*) in Ihrer Implementierung durch tats√§chliche ersetzen.

**Um mit der Indexerstellung fortzufahren:**

1. √ñffnen Sie die OpenSearch Dashboards und navigieren Sie zum Abschnitt DevTools.
2. Geben Sie im DevTools-Konsolenfenster die folgenden Abfragen ein und f√ºhren Sie sie aus, um Ihren Index mit dem benutzerdefinierten deutschen Analyzer zu erstellen:

```json
PUT /german_index
{
  "settings": {
    "index": {
      "analysis": {
        "analyzer": {
          "german_improved": {
            "tokenizer": "standard",
            "filter": [
              "lowercase",
              "german_decompounder",
              "german_stop",
              "german_stemmer"
            ]
          },
          "german_synonyms": {
            "tokenizer": "standard",
            "filter": [
              "lowercase",
              "german_decompounder",
              "synonym",
              "german_stop",
              "german_stemmer"
            ]
          }
        },
        "filter": {
          "synonym": {
            "type": "synonym",
            "synonyms_path": "analyzers/FYYYYYYY"
          },
          "german_decompounder": {
            "type": "hyphenation_decompounder",
            "word_list_path": "analyzers/FXXXXXXX",
            "hyphenation_patterns_path": "analyzers/FZZZZZZZ",
            "only_longest_match": false,
            "min_subword_size": 3
          },
          "german_stemmer": {
            "type": "stemmer",
            "language": "light_german"
          },
          "german_stop": {
            "type": "stop",
            "stopwords": "_german_",
            "remove_trailing": false
          }
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "paragraph": {
        "type": "text",
        "analyzer": "german_improved",
        "search_analyzer": "german_synonyms"
      }
    }
  }
}
```

In dieser Einrichtung verwenden wir zwei verschiedene Analyzer: 'german_improved' f√ºr die Indizierung und 'german_synonyms' f√ºr die Suche. Dies soll sowohl die Speicher- als auch die Suche effizienz verbessern.

- **Indizierung mit dem 'german_improved' Analyzer**: W√§hrend der Indizierung wird der 'german_improved' Analyzer verwendet. Dieser Analyzer umfasst einen Standard-Tokenizer und eine Reihe von Filtern einschlie√ülich Lowercase, german_decompounder, german_stop und german_stemmer. Das Hauptziel hier ist es, den Text zu dekomponieren und zu standardisieren, um die Konsistenz und Relevanz des Index zu verbessern. Wichtig ist, dass dieser Analyzer bewusst keinen Synonymfilter verwendet, um einen schlankeren und kompakteren Index zu erhalten, der sich auf die grundlegenden sprachlichen Elemente der deutschen Sprache konzentriert, ohne die zus√§tzliche Komplexit√§t und den Speicherbedarf von Synonymen.
- **Suche mit dem 'german_synonyms' Analyzer**: F√ºr die Suche wechseln wir zum 'german_synonyms' Analyzer. Dieser Analyzer hat dieselben Grundkomponenten wie 'german_improved', f√ºgt jedoch eine entscheidende Ebene hinzu - den Synonymfilter. Dies erh√∂ht die Flexibilit√§t und Relevanz der Suche erheblich, indem eine Reihe von synonymen Begriffen ber√ºcksichtigt wird, wodurch der Suchbereich erweitert wird, ohne die Genauigkeit zu beeintr√§chtigen.
- **Effizienz und Leistung**: Eines der bemerkenswerten Ergebnisse dieser Methodik ist die erhebliche Reduzierung der Indexgr√∂√üe ‚Äì mindestens 50% kleiner im Vergleich zur Verwendung von ngrams. Das Ausma√ü dieser Reduzierung kann je nach ngram-Bereich variieren. Dieser schlankere Index spart nicht nur Speicherplatz, sondern tr√§gt auch zu schnelleren Suchvorg√§ngen bei. Dar√ºber hinaus gew√§hrleistet die ausschlie√üliche Verwendung von Synonymen f√ºr die Suchphase, dass der Index fokussiert und effizient bleibt, w√§hrend die Suchvorg√§nge inklusiver und kontextuell bewusster werden.
- **Benutzerdefinierte Filter und Dekompoundierung**: Die benutzerdefinierten Filter wie 'german_decompounder', 'german_stemmer' und 'german_stop' sind speziell auf die einzigartigen Merkmale der deutschen Sprache zugeschnitten, wie z.B. zusammengesetzte W√∂rter und unterschiedliche Flexionen. Der Dekompoundierer ist insbesondere ein leistungsf√§higes Werkzeug, um komplexe deutsche Zusammensetzungen in besser suchbare Elemente zu zerlegen und so sowohl die Indizierung als auch die Suchprozesse weiter zu verfeinern.

Durch den Einsatz dieser Dual-Analyzer-Strategie erreichen wir ein optimales Gleichgewicht zwischen einem schlanken, effizienten Index und einer robusten, nuancierten Suchf√§higkeit, die speziell auf den deutschen Sprachkontext zugeschnitten ist.

**Jetzt k√∂nnen Sie den "german_improved" Analyzer ausprobieren:**

```json
GET german_index/_analyze?pretty
{
  "analyzer": "german_improved", 
  "text": ["Lebensversicherungsgesellschaft."]
}
```

**Sie k√∂nnen einige Dokumente zur Suche hinzuf√ºgen und weiter mit dem neuen Index experimentieren.**

```json
PUT /german_index/_doc/1
{
  "paragraph": "Eine Alsterrundfahrt bietet eine einzigartige Gelegenheit, die idyllische Landschaft und die st√§dtische Sch√∂nheit Hamburgs vom Wasser aus zu erleben."
} 
```

Nun werden Sie feststellen, dass die Suche dieses Dokument abruft, wenn Sie das Synonym "**Reise**" (bedeutet Reise, Fahrt, Trip) f√ºr einen Teil des Wortes "**Alsterrundfahrt**" (Alster Fluss Rundfahrt) verwenden, speziell f√ºr "**Rundfahrt**" (Rundfahrt).

```json
GET german_index/_search
{
  "query": {
    "match": {
      "paragraph": "Reise"
    }
  }
}
```

## Fazit

Die Verbesserungen in Amazon OpenSearch mit benutzerdefinierten Textanalyzern zeigen seine Anpassungsf√§higkeit und Effizienz bei der Bew√§ltigung deutscher Suchherausforderungen. Dieser Ansatz, der ein nuanciertes Verst√§ndnis von Sprachfeinheiten zeigt, ist f√ºr Unternehmen und Entwickler, die sich mit Volltextsuche befassen, von unsch√§tzbarem Wert und verbessert sowohl die Benutzererfahrung als auch die Suchrelevanz.

F√ºr diejenigen, die Alternativen suchen, kann das Erforschen von semantischen Suchtechniken und hybriden Ans√§tzen von Vorteil sein. Die semantische Suche dringt tiefer in das Verst√§ndnis des Kontexts und der Bedeutung hinter Benutzeranfragen ein und bietet eine anspruchsvollere Ebene der Suchgenauigkeit. Ein hybrider Ansatz, der die traditionelle Stichwortsuche mit semantischen F√§higkeiten kombiniert, kann die Ergebnisse weiter verfeinern, insbesondere in komplexen Suchszenarien.

Wir ermutigen Unternehmen und Entwickler, diese Verbesserungen in ihren Amazon OpenSearch-Bereitstellungen zu erforschen, um die verbesserten Suchf√§higkeiten und betrieblichen Effizienzen aus erster Hand zu erleben.

Wir bei tecRacer k√∂nnen Ihnen bei der Optimierung des Amazon OpenSearch Service helfen. Als [Amazon OpenSearch Service Delivery Partner](https://www.tecracer.com/de/consulting/amazon-opensearch-service/) bieten wir spezialisierte Unterst√ºtzung im Infrastrukturdesign, Automatisierung Ihres Cluster-Deployments und -Managements, Monitoring und Suchoptimierung.

‚Äî [Alexey](https://www.linkedin.com/comm/mynetwork/discovery-see-all?usecase=PEOPLE_FOLLOWS&followMember=vidanov)